<!DOCTYPE html>
<html lang="en"><head>
  <title>Bootstrap Website Tutorial</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
<link rel="stylesheet" href="../static/css/base.css">
</head>
<body>
	<nav class="navbar navbar-inverse">
		<div class="container-fluid">
			<div class="navbar-header">
				<button type="button" class="navbar-toggle" data-toggle="collapse" data-target="MyNavbar">
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
					<span class="icon-bar"></span>
				</button>
			</div>
			<div class="collapse navbar-collapse" id="myNavbar">
				<ul class="nav navbar-nav navbar-right">
          <li><a href="/">Home</a></li>
					<li class="active"><a href="/recorder">Record</a></li>
            <li><a href="/team">Team</a></li>
  					<li><a href="/about">About</a></li>
				</ul>
			</div>
		</div>
	</nav>

	<div class =  "recordPage">
		<h1>Example: Sample Recording</h1>
		<h3>Click "start recording".</h3>
		<h3>Click stop recording to complete the recording of your audio.</h3>
		<h3></h3>

		<!-- Draw the action buttons -->
		<button id="start-btn">Start recording</button>
		<button id="stop-btn" disabled>Stop recording</button>

		<!-- List item to store the recording files so they can be played in the browser -->
		<h2>Stored Recordings</h2>
		<ul id="recordingslist"></ul>

		<h2>Speech to Text</h2>
	

		<script>
			// Expose globally your audio_context, the recorder instance and audio_stream
			var audio_context;
			var recorder;
			var audio_stream;

			/**
			 * Patch the APIs for every browser that supports them and check
			 * if getUserMedia is supported on the browser.
			 *
			 */
			function Initialize() {
				try {
					// Monkeypatch for AudioContext, getUserMedia and URL
					window.AudioContext = window.AudioContext || window.webkitAudioContext;
					navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia;
					window.URL = window.URL || window.webkitURL;

					// Store the instance of AudioContext globally
					audio_context = new AudioContext;
					console.log('Audio context is ready !');
					console.log('navigator.getUserMedia ' + (navigator.getUserMedia ? 'available.' : 'not present!'));
				} catch (e) {
					alert('No web audio support in this browser!');
				}
			}

			/**
			 * Starts the recording process by requesting the access to the microphone.
			 * Then, if granted proceed to initialize the library and store the stream.
			 *
			 * It only stops when the method stopRecording is triggered.
			 */
			function startRecording() {
				// Access the Microphone using the navigator.getUserMedia method to obtain a stream
				navigator.getUserMedia({ audio: true }, function (stream) {
					// Expose the stream to be accessible globally
					audio_stream = stream;
					// Create the MediaStreamSource for the Recorder library
					var input = audio_context.createMediaStreamSource(stream);
					console.log('Media stream succesfully created');

					// Initialize the Recorder Library
					recorder = new Recorder(input);
					console.log('Recorder initialised');

					// Start recording !
					recorder && recorder.record();
					console.log('Recording...');

					// Disable Record button and enable stop button !
					document.getElementById("start-btn").disabled = true;
					document.getElementById("stop-btn").disabled = false;
				}, function (e) {
					console.error('No live audio input: ' + e);
				});
			}

			/**
			 * Stops the recording process. The method expects a callback as first
			 * argument (function) executed once the AudioBlob is generated and it
			 * receives the same Blob as first argument. The second argument is
			 * optional and specifies the format to export the blob either wav or mp3
			 Blob is an binary large object--mainly an audio or image for size
			 */
			function stopRecording(callback, AudioFormat) {
				// Stop the recorder instance
				recorder && recorder.stop();
				console.log('Stopped recording.');

				// Stop the getUserMedia Audio Stream !
				audio_stream.getAudioTracks()[0].stop();

				// Disable Stop button and enable Record button !
				document.getElementById("start-btn").disabled = false;
				document.getElementById("stop-btn").disabled = true;

				// Use the Recorder Library to export the recorder Audio as a .wav file
				// The callback providen in the stop recording method receives the blob
				if(typeof(callback) == "function"){

					/**
					 * Export the AudioBLOB using the exportWAV method.
					 * Note that this method exports too with mp3 if
					 * you provide the second argument of the function
					 */
					recorder && recorder.exportWAV(function (blob) {
						callback(blob);

						// create WAV download link using audio data blob
						// createDownloadLink();

						// Clear the Recorder to start again !
						recorder.clear();
					}, (AudioFormat || "audio/wav"));
				}
			}

			// Initialize everything once the window loads
			window.onload = function(){
				// Prepare and check if requirements are filled
				Initialize();

				// Handle on start recording button
				document.getElementById("start-btn").addEventListener("click", function(){
					startRecording();
					startButton(event);//this is where the speech-to-text call is happening
				}, false);

				// Handle on stop recording button
				document.getElementById("stop-btn").addEventListener("click", function(){
					// Use wav format
					recognizing=true;//recognizing is set true to stop recording
					startButton(event);//this is where the speech-to-text call is happening

					var _AudioFormat = "audio/wav";
					// You can use mp3 to using the correct mimetype
					//var AudioFormat = "audio/mpeg";

					stopRecording(function(AudioBLOB){
						// Note:
						// Use the AudioBLOB for whatever you need, to download
						// directly in the browser, to upload to the server, you name it !

						// In this case we are going to add an Audio item to the list so you
						// can play every stored Audio
						var url = URL.createObjectURL(AudioBLOB);
						var li = document.createElement('li');
						var au = document.createElement('audio');
						var hf = document.createElement('a');

						au.controls = true;
						au.src = url;
						hf.href = url;


						// Important:
						// Change the format of the file according to the mimetype
						// e.g for audio/wav the extension is .wav
						//     for audio/mpeg (mp3) the extension is .mp3
						hf.download = new Date().toISOString() + '.wav';
						hf.innerHTML = hf.download;
						li.appendChild(au);
						li.appendChild(hf);
						recordingslist.appendChild(li);			
						}, _AudioFormat);


				}, false);
				};

		</script>

		<!-- THIS IS THE SECTION WHERE Speech Recognition STARTS -->
	
 		<div>
  		<a href="#" id="start_button" onclick="startButton(event)">Transcript</a>
		</div>


		<div id="results">
		  <span id="final_span" class="final"></span>
		  <span id="interim_span" class="interim"></span>
		</div>

		<script type="text/javascript">
		var final_transcript = '';
		var recognizing = false;


		if ('webkitSpeechRecognition' in window) {

		  var recognition = new webkitSpeechRecognition();


		  recognition.continuous = true;
		  recognition.interimResults = true;

		  recognition.onstart = function() {
		    recognizing = true;
		  };

		  recognition.onerror = function(event) {
		    console.log(event.error);
		  };

		  recognition.onend = function() {
		    recognizing = false;
		  };

		  recognition.onresult = function(event) {
		    var interim_transcript = '';
		    for (var i = event.resultIndex; i < event.results.length; ++i) {
		      if (event.results[i].isFinal) {
		        final_transcript += event.results[i][0].transcript;
		      } else {
		        interim_transcript += event.results[i][0].transcript;
		      }
		    }
		    final_transcript = capitalize(final_transcript);
		    final_span.innerHTML = linebreak(final_transcript);
		    interim_span.innerHTML = linebreak(interim_transcript);
		    
		  };
		}

		var two_line = /\n\n/g;
		var one_line = /\n/g;
		function linebreak(s) {
		  return s.replace(two_line, '<p></p>').replace(one_line, '<br>');
		}

		function capitalize(s) {
		  return s.replace(s.substr(0,1), function(m) { return m.toUpperCase(); });
		}

		function startButton(event) {
		  if (recognizing) {
		    recognition.stop();
		    return;
		  }
		  final_transcript = '';
		  recognition.lang = 'en-US';
		  recognition.start();
		  final_span.innerHTML = '';
		  interim_span.innerHTML = '';
		}
		</script>
		<script src="../static/js/recorder.js"></script>


	</div>
</body>
</html>